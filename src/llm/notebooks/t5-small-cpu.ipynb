{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f826a733-3e08-453a-81fd-678f112d86dd",
   "metadata": {},
   "source": [
    "# Using T5 Small for Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdc4c7-d871-4616-b5da-c3c3c08546eb",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eea0eeb-c3a8-4f24-9b27-11f7f01ee8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.11/site-packages (4.30.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: sentencepiece in /home/jovyan/.local/lib/python3.11/site-packages (0.1.99)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.7.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate transformers[torch] torch sentencepiece --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d91cd-1bbe-4b38-8128-4427974349e8",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084571f-09c8-49da-977e-ff56f32885f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, T5Tokenizer, T5ForConditionalGeneration, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa867a4-de07-46b8-a9aa-f33b7bc1f65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71a647234334f408cd970259bdf50a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed571aac72b14b1b817c22996ca6e087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b325e580512043deaa8febb3c9932f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c294c559467543d7b5554004ad6a294c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22af698a832147c48da18324749c1add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d378a3583ca4884ac883fae02cf31f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie ich er bitten?</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46167f1c-bb32-4ae3-817d-2e97622bc316",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Storing Model [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "228ff887-73ab-4ab4-83cf-81af7c6cc642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully!\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# uncomment if you want to store your models to a folder\n",
    "# model_name = model_id.replace('/', '_')\n",
    "# models_path = Path(f\"{os.getcwd()}/models/{model_name}\")\n",
    "# if not models_path.exists():\n",
    "#  models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Storing tokenizer locally\n",
    "# tokenizer.save_pretrained(str(models_path))\n",
    "# # Storing model locally\n",
    "# model.save_pretrained(str(models_path))\n",
    "# print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b516f6-8428-4cd0-a776-8dc1f0f51209",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading stored model [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e93e8951-e72e-47f1-a499-4aa8167c1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you want to load your stored model\n",
    "# model_name = model_id.replace('/', '_')\n",
    "# models_path = Path(f\"{os.getcwd()}/models/{model_name}\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(str(models_path))\n",
    "# model = T5ForConditionalGeneration.from_pretrained(str(models_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85cc38-e6a5-4f5a-a82e-d8326263aa0c",
   "metadata": {},
   "source": [
    "## Accessing Embeddings Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf61335-d3c9-4314-97f5-ce363fe1690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\"./db/\"\n",
    "))\n",
    "collection = client.get_collection(name=\"airflow_docs_stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "486d14df-7a67-44ba-b130-dde62d2b8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dag_loader.py¶  from airflow import DAG  from airflow.decorators import task   import pendulum    def create_dag(dag_id, schedule, dag_number, default_args):      dag = DAG(          dag_id,          schedule=schedule,          default_args=default_args,          pendulum.datetime(2021, 9, 13, tz=\"UTC\"),      )       with dag:           @task()          def hello_world():              print(\"Hello World\")              print(f\"This is DAG: {dag_number}\")           hello_world()       return dag       DAG construction¶\n"
     ]
    }
   ],
   "source": [
    "question = \"Python Code to create a Dag Class\"\n",
    "results = collection.query(\n",
    "    query_texts=[question],\n",
    "    n_results=1,\n",
    ")\n",
    "formatted_result = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "print(formatted_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3a03f-7655-459f-8fc0-73ffd8fad2dc",
   "metadata": {},
   "source": [
    "## Setting up Retrieval Augmeneted Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1b364fd-5ec7-439f-b158-96606b45ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful question and answer bot, your task is to provide the best answer to a given user's question.\n",
      "Only use the context below to answer the user's question, if you don't have the necessary information to answer say: 'I don't know!'\n",
      "Context and Question are denoted by ```\n",
      "Context: ```dag_loader.py¶  from airflow import DAG  from airflow.decorators import task   import pendulum    def create_dag(dag_id, schedule, dag_number, default_args):      dag = DAG(          dag_id,          schedule=schedule,          default_args=default_args,          pendulum.datetime(2021, 9, 13, tz=\"UTC\"),      )       with dag:           @task()          def hello_world():              print(\"Hello World\")              print(f\"This is DAG: {dag_number}\")           hello_world()       return dag       DAG construction¶```\n",
      "\n",
      "Question: ```Python Code to create a Dag Class?```\n",
      "\n",
      "Response: ```\n",
      "\n",
      "Parameters:\n",
      "\n",
      "dag - a DAG object - the DAG method to create.\n",
      "\n",
      "default - the default DAG function that returns the specified DAG object or function argument (see below).\n",
      "\n",
      "parameter_list - the dictionary of DAG parameters.\n",
      "\n",
      "args - the dictionaries of the DAG parameters.\n",
      "\n",
      "Returns: - an instance DAG object that accepts a Dag method or an object from a specified DAG array.\n",
      "\n",
      "class_parameter_list - the dictionary of parameters.\n",
      "\n",
      "Returns: - a DAG object with the following parameters: parameters: [ 'dag_id', 'default', 'tz', 'UTC', 'tz', 'default',\n",
      "\n",
      "]\n",
      "\n",
      "Returns: [1] objects with the following parameters: parameters: [dag]\n",
      "\n",
      "-- the default dag function - a DAG object to create. ```\n",
      "\n",
      "Args: [ 'dag', 'default', 'tz', 'UTC', 'tz', 'default',\n",
      "\n",
      "]\n",
      "\n",
      "Returns: undefined if Dag is not required while creating the class: no arguments to this function.\n",
      "\n",
      "class_parameter_list - the dict of DAG parameters.\n",
      "\n",
      "\n",
      "Returns: undefined if Dag is not required while creating the class: no arguments to this function.\n",
      "\n",
      "instance_parameter_list - the dictionary of parameters.\n",
      "\n",
      "\n",
      "Returns: undefined if Dag is required when creating a class. ```\n",
      "\n",
      "Parameters: ```.\n",
      "\n",
      "-- the instance DAG object to bind to - set a custom bind_parameter.\n",
      "\n",
      "-- set a custom bind_parameter. [, dag ]\n",
      "\n",
      "Returns: [ :dag, :tz ]\n",
      "\n",
      "You Can See DAG: DAG at GitHub Pages:\n",
      "\n",
      "https://github.com/dag/dag\n",
      "\n",
      "DAG.module.name.starttime.\n",
      "\n",
      "DAG.add_to_range. function() {\n",
      "\n",
      "this[ 'dag' ] = function() {};\n",
      "\n",
      "this[ 'add_to_range' ] ;\n",
      "\n",
      "};\n",
      "\n",
      "add_to_range.add_to_range(this,'s', 'this' );\n",
      "\n",
      "-- Get the distance from the destination and return that.\n",
      "\n",
      "dag = dag_get_distance(this, this,'s', 'this\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"You are a helpful question and answer bot, your task is to provide the best answer to a given user's question.\\n\"\n",
    "    \"Only use the context below to answer the user's question, if you don't have the necessary information to answer say: 'I don't know!'\\n\"\n",
    "    \"Context and Question are denoted by ```\\n\"\n",
    "    f\"Context: ```{formatted_result}```\\n\\n\"\n",
    "    f\"Question: ```{question}?```\\n\\n\"\n",
    "    \"Response:\"\n",
    ")\n",
    "# response = text_generation(prompt) # WIP: add text generation pipeline\n",
    "# print(response[0][\"generated_text\"].lstrip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
