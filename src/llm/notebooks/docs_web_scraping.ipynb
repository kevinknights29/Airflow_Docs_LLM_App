{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c368f6f-4196-45ed-91f8-bd83d6e861fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T06:28:31.309980Z",
     "iopub.status.busy": "2023-07-22T06:28:31.309754Z",
     "iopub.status.idle": "2023-07-22T06:28:33.039571Z",
     "shell.execute_reply": "2023-07-22T06:28:33.038909Z",
     "shell.execute_reply.started": "2023-07-22T06:28:31.309959Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: langchain in /home/jovyan/.local/lib/python3.11/site-packages (0.0.239)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jovyan/.local/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/jovyan/.local/lib/python3.11/site-packages (from langchain) (0.5.13)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /home/jovyan/.local/lib/python3.11/site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/jovyan/.local/lib/python3.11/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/jovyan/.local/lib/python3.11/site-packages (from langchain) (1.10.11)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jovyan/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jovyan/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jovyan/.local/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jovyan/.local/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 requests pandas langchain --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679b43a0-1fb1-4b8d-93da-11d9b7ee1d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T06:28:33.041206Z",
     "iopub.status.busy": "2023-07-22T06:28:33.041015Z",
     "iopub.status.idle": "2023-07-22T06:28:33.706233Z",
     "shell.execute_reply": "2023-07-22T06:28:33.705726Z",
     "shell.execute_reply.started": "2023-07-22T06:28:33.041186Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://airflow.apache.org/docs/apache-airflow/stable/migrations-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/project.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/ui.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/extra-packages-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/license.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/security/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/howto/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/database-erd-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/integration.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/privacy_notice.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/deprecated-rest-api-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/faq.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/index.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/start.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/public-airflow-interface.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html\n",
      "https://airflow.apache.org/docs/apache-airflow/stable/release-process.html\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "root_url = \"https://airflow.apache.org/docs/apache-airflow/stable/\"\n",
    "root_response = requests.get(root_url)\n",
    "root_html = root_response.content.decode(\"utf-8\")\n",
    "soup = BeautifulSoup(root_html, \"html.parser\")\n",
    "\n",
    "root_url_parts = urlparse(root_url)\n",
    "root_links = soup.find_all(\"a\", attrs={\"class\": \"reference internal\"})\n",
    "\n",
    "result = set()\n",
    "for root_link in root_links:\n",
    "    path = root_url_parts.path + root_link.get(\"href\")\n",
    "    path = str(Path(path).resolve())\n",
    "    path = urlparse(path).path\n",
    "    url = f\"{root_url_parts.scheme}://{root_url_parts.netloc}{path}\"\n",
    "    result.add(url)\n",
    "urls = list(result)\n",
    "print(*urls, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d5c583-258d-4c6e-9c70-e3368497f856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T06:37:21.691742Z",
     "iopub.status.busy": "2023-07-22T06:37:21.690111Z",
     "iopub.status.idle": "2023-07-22T06:37:22.137549Z",
     "shell.execute_reply": "2023-07-22T06:37:22.136652Z",
     "shell.execute_reply.started": "2023-07-22T06:37:21.691675Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n\\n\\n  \\n\\n\\n\\n\\nWhat is Airflow™? — Airflow Documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            Community\\n                        \\n\\n                            Meetups\\n                        \\n\\n                            Documentation\\n                        \\n\\n                            Use-cases\\n                        \\n\\n                            Announcements\\n                        \\n\\n                            Blog\\n                        \\n\\n                            Ecosystem\\n                        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                Community\\n                            \\n\\n                                Meetups\\n                            \\n\\n                                Documentation\\n                            \\n\\n                                Use-cases\\n                            \\n\\n                                Announcements\\n                            \\n\\n                                Blog\\n                            \\n\\n                                Ecosystem\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContent\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVersion: 2.6.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContent\\n\\nOverview\\nQuick Start\\nInstallation of Airflow™\\nSecurity\\nTutorials\\nHow-to Guides\\nUI / Screenshots\\nCore Concepts\\nAuthoring and Scheduling\\nAdministration and Deployment\\nIntegration\\nPublic Interface of Airflow\\nBest Practices\\nFAQ\\nRelease Policies\\nRelease Notes\\nPrivacy Notice\\nProject\\nLicense\\n\\nReferences\\n\\nOperators and hooks\\nCLI\\nTemplates\\nStable REST API\\nDeprecated REST API\\nConfigurations\\nExtra packages\\n\\nInternal DB details\\n\\nDatabase Migrations\\nDatabase ERD Schema\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVersion: 2.6.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContent\\n\\nOverview\\nQuick Start\\nInstallation of Airflow™\\nSecurity\\nTutorials\\nHow-to Guides\\nUI / Screenshots\\nCore Concepts\\nAuthoring and Scheduling\\nAdministration and Deployment\\nIntegration\\nPublic Interface of Airflow\\nBest Practices\\nFAQ\\nRelease Policies\\nRelease Notes\\nPrivacy Notice\\nProject\\nLicense\\n\\nReferences\\n\\nOperators and hooks\\nCLI\\nTemplates\\nStable REST API\\nDeprecated REST API\\nConfigurations\\nExtra packages\\n\\nInternal DB details\\n\\nDatabase Migrations\\nDatabase ERD Schema\\n\\n\\n\\n\\n\\n\\n\\n Home\\n What is Airflow™?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Airflow™?¶\\nApache Airflow™ is an open-source platform for developing, scheduling,\\nand monitoring batch-oriented workflows. Airflow’s extensible Python framework enables you to build workflows\\nconnecting with virtually any technology. A web interface helps manage the state of your workflows. Airflow is\\ndeployable in many ways, varying from a single process on your laptop to a distributed setup to support even\\nthe biggest workflows.\\n\\n\\nWorkflows as code¶\\nThe main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes:\\n\\nDynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation.\\nExtensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment.\\nFlexible: Workflow parameterization is built-in leveraging the Jinja templating engine.\\n\\nTake a look at the following snippet of code:\\nfrom datetime import datetime\\n\\nfrom airflow import DAG\\nfrom airflow.decorators import task\\nfrom airflow.operators.bash import BashOperator\\n\\n# A DAG represents a workflow, a collection of tasks\\nwith DAG(dag_id=\"demo\", start_date=datetime(2022, 1, 1), schedule=\"0 0 * * *\") as dag:\\n\\n    # Tasks are represented as operators\\n    hello = BashOperator(task_id=\"hello\", bash_command=\"echo hello\")\\n\\n    @task()\\n    def airflow():\\n        print(\"airflow\")\\n\\n    # Set dependencies between tasks\\n    hello >> airflow()\\n\\n\\nHere you see:\\n\\nA DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow.\\nTwo tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator\\n>> between the tasks defines a dependency and controls in which order the tasks will be executed\\n\\nAirflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface:\\n\\nThis example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time:\\n\\nEach column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.\\n\\n\\nWhy Airflow™?¶\\nAirflow™ is a batch workflow orchestration platform. The Airflow framework contains operators to connect with\\nmany technologies and is easily extensible to connect with a new technology. If your workflows have a clear\\nstart and end, and run at regular intervals, they can be programmed as an Airflow DAG.\\nIf you prefer coding over clicking, Airflow is the tool for you. Workflows are defined as Python code which\\nmeans:\\n\\nWorkflows can be stored in version control so that you can roll back to previous versions\\nWorkflows can be developed by multiple people simultaneously\\nTests can be written to validate functionality\\nComponents are extensible and you can build on a wide collection of existing components\\n\\nRich scheduling and execution semantics enable you to easily define complex pipelines, running at regular\\nintervals. Backfilling allows you to (re-)run pipelines on historical data after making changes to your logic.\\nAnd the ability to rerun partial pipelines after resolving an error helps maximize efficiency.\\nAirflow’s user interface provides:\\n\\n\\nIn-depth views of two things:\\n\\n\\n\\nPipelines\\nTasks\\n\\n\\n\\nOverview of your pipelines over time\\n\\n\\nFrom the interface, you can inspect logs and manage tasks, for example retrying a task in\\ncase of failure.\\nThe open-source nature of Airflow ensures you work on components developed, tested, and used by many other\\ncompanies around the world. In the active\\ncommunity you can find plenty of helpful resources in the form of\\nblog posts, articles, conferences, books, and more. You can connect with other peers via several channels\\nsuch as Slack and mailing lists.\\nAirflow as a Platform is highly customizable. By utilizing Public Interface of Airflow you can extend\\nand customize almost every aspect of Airflow.\\n\\n\\nWhy not Airflow™?¶\\nAirflow™ was built for finite batch workflows. While the CLI and REST API do allow triggering workflows,\\nAirflow was not built for infinitely running event-based workflows. Airflow is not a streaming solution.\\nHowever, a streaming system such as Apache Kafka is often seen working together with Apache Airflow. Kafka can\\nbe used for ingestion and processing in real-time, event data is written to a storage location, and Airflow\\nperiodically starts a workflow processing a batch of data.\\nIf you prefer clicking over coding, Airflow is probably not the right solution. The web interface aims to make\\nmanaging workflows as easy as possible and the Airflow framework is continuously improved to make the\\ndeveloper experience as smooth as possible. However, the philosophy of Airflow is to define workflows as code\\nso coding will always be required.\\n\\n\\n\\n\\n\\n\\n\\nPrevious\\n\\nNext\\n\\n\\n\\n\\n\\n\\n\\n\\nWas this entry helpful?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Airflow™?\\nWorkflows as code\\nWhy Airflow™?\\nWhy not Airflow™?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSuggest a change on this page\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWant to be a part of Apache Airflow?\\n\\nJoin community\\n\\n\\n\\n\\n\\n© The Apache Software Foundation \\n\\n\\nLicense\\n\\n\\nDonate\\n\\n\\nThanks\\n\\n\\nSecurity\\n\\n\\n\\n\\n            Apache Airflow, Apache, Airflow, the Airflow logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation.\\n            All other products or name brands are trademarks of their respective holders, including The Apache Software Foundation.\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://airflow.apache.org/docs/apache-airflow/stable/', 'title': 'What is Airflow™? — Airflow Documentation', 'language': 'en'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://airflow.apache.org/docs/apache-airflow/stable/\")\n",
    "documents = loader.load()\n",
    "documents[0]\n",
    "# Conclusion: This loads a lot of unnecesary text and is poorly formated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d0d5e6c-6fc9-4a5f-b0de-c57037aa9a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T07:10:05.796228Z",
     "iopub.status.busy": "2023-07-22T07:10:05.794595Z",
     "iopub.status.idle": "2023-07-22T07:10:06.114959Z",
     "shell.execute_reply": "2023-07-22T07:10:06.114153Z",
     "shell.execute_reply.started": "2023-07-22T07:10:05.796169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\n",
      "code” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\n",
      "of the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\n",
      "of running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\n",
      "seen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\n",
      "other views which allow you to deep dive into the state of your workflows.\n",
      "Char Length: 1460\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Code: # python\n",
      "# code-snippet-start\n",
      "from datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\n",
      "# code-snippet-end\n",
      "Char Length: 550\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://airflow.apache.org/docs/apache-airflow/stable/\")\n",
    "html = response.content.decode(\"utf-8\")\n",
    "soup = BeautifulSoup(root_html, \"html.parser\")\n",
    "\n",
    "# Find all divs with class=\"section\"\n",
    "divs = soup.find_all(\"div\", class_=\"section\")\n",
    "\n",
    "# Extract the text from each div that contains a 'pre' element\n",
    "divs_with_pre = [div for div in divs if div.find(\"pre\")]\n",
    "\n",
    "# Example of a div:\n",
    "example_div = divs_with_pre[0]\n",
    "\n",
    "# Find all p tags within this div\n",
    "p_tags = example_div.find_all(\"p\")\n",
    "p_texts = [p_tag.text.strip() for p_tag in p_tags]\n",
    "all_p_text = \" \".join(p_texts)\n",
    "\n",
    "# Find all pre tags within this div\n",
    "pre_tags = example_div.find_all(\"pre\")\n",
    "all_span_text = \"\"\n",
    "code_snippet_start = \"# python\\n# code-snippet-start\\n\"\n",
    "code_snippet_end = \"\\n# code-snippet-end\"\n",
    "for pre_tag in pre_tags:\n",
    "    span_tags = pre_tag.find_all(\"span\")\n",
    "    span_texts = [span_tag.text for span_tag in span_tags]\n",
    "    all_span_text = \" \".join(span_texts)\n",
    "    all_span_text = (\n",
    "        \"# python\\n# code-snippet-start\\n\"\n",
    "        f\"{all_span_text.strip()}\"\n",
    "        \"\\n# code-snippet-end\"\n",
    "    )\n",
    "\n",
    "print(\"Text:\", all_p_text)\n",
    "print(f\"Char Length: {len(all_p_text)}\")\n",
    "print(\"\", \"=\" * 100, \"\", sep=\"\\n\")\n",
    "print(\"Code:\", all_span_text)\n",
    "print(f\"Char Length: {len(all_span_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cab8781-3011-4c08-a87a-a601e178e44c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T07:26:35.582873Z",
     "iopub.status.busy": "2023-07-22T07:26:35.581543Z",
     "iopub.status.idle": "2023-07-22T07:26:35.595259Z",
     "shell.execute_reply": "2023-07-22T07:26:35.593220Z",
     "shell.execute_reply.started": "2023-07-22T07:26:35.582823Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _parse_url(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.content.decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(root_html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_sections(url, exclude_no_code=True):\n",
    "    soup = _parse_url(url)\n",
    "    # Find all divs with class=\"section\"\n",
    "    divs = soup.find_all(\"div\", class_=\"section\")\n",
    "    if exclude_no_code:\n",
    "        # Extract the text from each div that contains a 'pre' element\n",
    "        code_divs = [div for div in divs if div.find(\"pre\")]\n",
    "        return code_divs\n",
    "    return divs\n",
    "\n",
    "\n",
    "def _extract_text_from_div(div):\n",
    "    # Find all p tags within this div\n",
    "    p_tags = div.find_all(\"p\")\n",
    "    p_texts = [p_tag.text.strip() for p_tag in p_tags]\n",
    "    all_p_text = \" \".join(p_texts)\n",
    "    return all_p_text\n",
    "\n",
    "\n",
    "def _extract_code_from_div(div):\n",
    "    # Find all pre tags within this div\n",
    "    pre_tags = div.find_all(\"pre\")\n",
    "    all_span_text = \"\"\n",
    "    code_snippet_start = \"# python\\n# code-snippet-start\\n\"\n",
    "    code_snippet_end = \"\\n# code-snippet-end\"\n",
    "    for pre_tag in pre_tags:\n",
    "        span_tags = pre_tag.find_all(\"span\")\n",
    "        span_texts = [span_tag.text for span_tag in span_tags]\n",
    "        all_span_text = \" \".join(span_texts)\n",
    "        all_span_text = (\n",
    "            \"# python\\n# code-snippet-start\\n\"\n",
    "            f\"{all_span_text.strip()}\"\n",
    "            \"\\n# code-snippet-end\"\n",
    "        )\n",
    "    return all_span_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87667f8f-568a-4781-89a2-f4d50c170191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T07:29:03.940745Z",
     "iopub.status.busy": "2023-07-22T07:29:03.939292Z",
     "iopub.status.idle": "2023-07-22T07:29:14.286509Z",
     "shell.execute_reply": "2023-07-22T07:29:14.285608Z",
     "shell.execute_reply.started": "2023-07-22T07:29:03.940688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/migrations-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/project.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/ui.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/extra-packages-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/license.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/security/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/howto/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/operators-and-hooks-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/database-erd-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/integration.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/privacy_notice.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/deprecated-rest-api-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/faq.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/index.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/start.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/public-airflow-interface.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n",
      "\n",
      "{'url': 'https://airflow.apache.org/docs/apache-airflow/stable/release-process.html', 'text': 'The main characteristic of Airflow workflows is that all workflows are defined in Python code. “Workflows as\\ncode” serves several purposes: Dynamic: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation. Extensible: The Airflow™ framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment. Flexible: Workflow parameterization is built-in leveraging the Jinja templating engine. Take a look at the following snippet of code: Here you see: A DAG named “demo”, starting on Jan 1st 2022 and running once a day. A DAG is Airflow’s representation of a workflow. Two tasks, a BashOperator running a Bash script and a Python function defined using the @task decorator >> between the tasks defines a dependency and controls in which order the tasks will be executed Airflow evaluates this script and executes the tasks at the set interval and in the defined order. The status\\nof the “demo” DAG is visible in the web interface: This example demonstrates a simple Bash and Python script, but these tasks can run any arbitrary code. Think\\nof running a Spark job, moving data between two buckets, or sending an email. The same structure can also be\\nseen running over time: Each column represents one DAG run. These are two of the most used views in Airflow, but there are several\\nother views which allow you to deep dive into the state of your workflows.', 'code': '# python\\n# code-snippet-start\\nfrom datetime import datetime from airflow import DAG from airflow.decorators import task from airflow.operators.bash import BashOperator # A DAG represents a workflow, a collection of tasks with DAG ( dag_id = \"demo\" , start_date = datetime ( 2022 , 1 , 1 ), schedule = \"0 0 * * *\" ) as dag : # Tasks are represented as operators hello = BashOperator ( task_id = \"hello\" , bash_command = \"echo hello\" ) @task () def airflow (): print ( \"airflow\" ) # Set dependencies between tasks hello >> airflow ()\\n# code-snippet-end'}\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for url in urls:\n",
    "    for section in extract_sections(url):\n",
    "        dataset.append(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"text\": _extract_text_from_div(section),\n",
    "                \"code\": _extract_code_from_div(section),\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"Results:\")\n",
    "print(*dataset, sep=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
